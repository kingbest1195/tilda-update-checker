"""
–ú–æ–¥—É–ª—å –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –Ω–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤ Tilda (Discovery Mode)
"""
import logging
import re
from typing import List, Dict, Set, Optional
from urllib.parse import urlparse, urljoin

import requests
from bs4 import BeautifulSoup

import config
from src.database import db
from src.version_detector import detector
from src.alert_system import alert_system

logger = logging.getLogger(__name__)


# –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
PATTERN_CATEGORIES = {
    r"tilda-members|members\.tildaapi\.com|members2\.tildacdn\.com": "members",
    r"tilda-(cart|catalog|wishlist|products|variant)": "ecommerce",
    r"tilda-zero": "zero_block",
    r"tilda-(quiz|cards|stories|slider|popup|slds|zoom|video)": "ui_components",
    r"tilda-(scripts|grid|forms|animation|cover|menu|lazyload|events)": "core",
    r"tilda-(phone|conditional|payment|ratescale|step-manager|widget|lk|skiplink|stat|error|performance|table|paint|redactor|highlight)": "utilities",
}

# –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç—ã –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π
CATEGORY_PRIORITIES = {
    "core": "CRITICAL",
    "members": "CRITICAL",
    "ecommerce": "HIGH",
    "zero_block": "HIGH",
    "ui_components": "MEDIUM",
    "utilities": "LOW",
    "unknown": "MEDIUM"
}

# –î–æ–º–µ–Ω—ã Tilda –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
TILDA_DOMAINS = [
    "static.tildacdn.com",
    "members.tildaapi.com",
    "members2.tildacdn.com",
    "neo.tildacdn.com",
]

# –°—Ç—Ä–∞–Ω–∏—Ü—ã-–∫–∞–Ω–∞—Ä–µ–π–∫–∏ –¥–ª—è —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
CANARY_PAGES = [
    "https://tilda.nomadnocode.com/all-external",
    "https://tilda.nomadnocode.com/members/login",
    "https://tilda.nomadnocode.com/members/recover-password",
]


class FileDiscovery:
    """–ö–ª–∞—Å—Å –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –Ω–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤"""
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è discovery"""
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': config.USER_AGENT
        })
    
    def discover_files(self) -> List[Dict]:
        """
        –û–±–Ω–∞—Ä—É–∂–∏—Ç—å –Ω–æ–≤—ã–µ —Ñ–∞–π–ª—ã –Ω–∞ –∫–∞–Ω–∞—Ä–µ–π–∫–∞-—Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö
        
        Returns:
            –°–ø–∏—Å–æ–∫ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã—Ö –Ω–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤
        """
        all_discovered = []
        already_tracked = self._get_tracked_urls()
        
        logger.info(f"–ù–∞—á–∞–ª–æ Discovery Mode. –£–∂–µ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ—Ç—Å—è —Ñ–∞–π–ª–æ–≤: {len(already_tracked)}")
        
        for page_url in CANARY_PAGES:
            try:
                logger.info(f"–°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {page_url}")
                discovered = self._scan_page(page_url, already_tracked)
                
                if discovered:
                    logger.info(f"  ‚Üí –ù–∞–π–¥–µ–Ω–æ –Ω–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤: {len(discovered)}")
                    all_discovered.extend(discovered)
                else:
                    logger.info(f"  ‚Üí –ù–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ")
                    
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–∏ {page_url}: {e}", exc_info=True)
        
        # –£–¥–∞–ª–∏—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã
        unique_discovered = self._remove_duplicates(all_discovered)
        
        logger.info(f"Discovery Mode –∑–∞–≤–µ—Ä—à–µ–Ω. –ù–∞–π–¥–µ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –Ω–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤: {len(unique_discovered)}")
        
        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ –ë–î
        for file_info in unique_discovered:
            self._save_discovered_file(file_info)
        
        return unique_discovered
    
    def _get_tracked_urls(self) -> Set[str]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –º–Ω–æ–∂–µ—Å—Ç–≤–æ —É–∂–µ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º—ã—Ö URL
        
        Returns:
            –ú–Ω–æ–∂–µ—Å—Ç–≤–æ URL
        """
        tracked = set()
        
        # –ò–∑ config
        for category_data in config.TILDA_MONITORED_FILES.values():
            tracked.update(category_data.get('files', []))
        
        # –ò–∑ –ë–î
        tracked_files = db.get_tracked_files()
        for tf in tracked_files:
            tracked.add(tf.url)
        
        return tracked
    
    def _scan_page(self, page_url: str, already_tracked: Set[str]) -> List[Dict]:
        """
        –°–∫–∞–Ω–∏—Ä–æ–≤–∞—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É –Ω–∞ –Ω–∞–ª–∏—á–∏–µ —Å—Å—ã–ª–æ–∫ –Ω–∞ Tilda —Ñ–∞–π–ª—ã
        
        Args:
            page_url: URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
            already_tracked: –ú–Ω–æ–∂–µ—Å—Ç–≤–æ —É–∂–µ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º—ã—Ö URL
            
        Returns:
            –°–ø–∏—Å–æ–∫ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        """
        try:
            response = self.session.get(page_url, timeout=30)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # –ù–∞–π—Ç–∏ –≤—Å–µ script –∏ link —Ç–µ–≥–∏
            discovered_urls = set()
            
            # Script —Ç–µ–≥–∏
            for script in soup.find_all('script', src=True):
                url = urljoin(page_url, script['src'])
                if self._is_tilda_url(url):
                    discovered_urls.add(url)
            
            # Link —Ç–µ–≥–∏ (CSS)
            for link in soup.find_all('link', href=True):
                if link.get('rel') == ['stylesheet'] or link['href'].endswith('.css'):
                    url = urljoin(page_url, link['href'])
                    if self._is_tilda_url(url):
                        discovered_urls.add(url)
            
            # –§–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å —É–∂–µ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º—ã–µ
            new_urls = discovered_urls - already_tracked
            
            # –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∏—Ä–æ–≤–∞—Ç—å
            discovered_files = []
            for url in new_urls:
                file_info = self._categorize_file(url, page_url)
                discovered_files.append(file_info)
            
            return discovered_files
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_url}: {e}")
            return []
    
    def _is_tilda_url(self, url: str) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –ª–∏ URL –∫ Tilda
        
        Args:
            url: URL –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
            
        Returns:
            True –µ—Å–ª–∏ —ç—Ç–æ Tilda URL
        """
        parsed = urlparse(url)
        domain = parsed.netloc
        
        return any(tilda_domain in domain for tilda_domain in TILDA_DOMAINS)
    
    def _categorize_file(self, url: str, source_page: str) -> Dict:
        """
        –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∏—Ä–æ–≤–∞—Ç—å –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
        
        Args:
            url: URL —Ñ–∞–π–ª–∞
            source_page: –°—Ç—Ä–∞–Ω–∏—Ü–∞, –Ω–∞ –∫–æ—Ç–æ—Ä–æ–π –±—ã–ª –æ–±–Ω–∞—Ä—É–∂–µ–Ω
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Ñ–∞–π–ª–µ
        """
        # –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏—é –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º
        category = "unknown"
        pattern_matched = None
        
        for pattern, cat in PATTERN_CATEGORIES.items():
            if re.search(pattern, url, re.IGNORECASE):
                category = cat
                pattern_matched = pattern
                break
        
        # –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
        priority = CATEGORY_PRIORITIES.get(category, "MEDIUM")
        
        # –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–∏–ø —Ñ–∞–π–ª–∞
        if url.endswith('.js'):
            file_type = 'js'
        elif url.endswith('.css'):
            file_type = 'css'
        else:
            file_type = 'unknown'
        
        # –ò–∑–≤–ª–µ—á—å –¥–æ–º–µ–Ω
        parsed = urlparse(url)
        domain = parsed.netloc
        
        return {
            'url': url,
            'category': category,
            'priority': priority,
            'file_type': file_type,
            'domain': domain,
            'source_page': source_page,
            'pattern_matched': pattern_matched
        }
    
    def _remove_duplicates(self, discovered: List[Dict]) -> List[Dict]:
        """
        –£–¥–∞–ª–∏—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã –∏–∑ —Å–ø–∏—Å–∫–∞ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        
        Args:
            discovered: –°–ø–∏—Å–æ–∫ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
            
        Returns:
            –°–ø–∏—Å–æ–∫ –±–µ–∑ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        """
        seen_urls = set()
        unique = []
        
        for file_info in discovered:
            url = file_info['url']
            if url not in seen_urls:
                seen_urls.add(url)
                unique.append(file_info)
        
        return unique
    
    def _save_discovered_file(self, file_info: Dict):
        """
        –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –≤ –ë–î
        
        Args:
            file_info: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ñ–∞–π–ª–µ
        """
        try:
            db.save_discovered_file(
                url=file_info['url'],
                source_page=file_info['source_page'],
                pattern_matched=file_info.get('pattern_matched'),
                suggested_category=file_info['category']
            )
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ {file_info['url']}: {e}")
    
    def get_new_discoveries(self) -> List[Dict]:
        """
        –ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –µ—â–µ –Ω–µ –¥–æ–±–∞–≤–ª–µ–Ω—ã –≤ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ
        
        Returns:
            –°–ø–∏—Å–æ–∫ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        """
        discovered_files = db.get_undiscovered_files()
        
        result = []
        for df in discovered_files:
            result.append({
                'id': df.id,
                'url': df.url,
                'category': df.suggested_category,
                'discovered_at': df.discovered_at,
                'source_page': df.source_page
            })
        
        return result
    
    def print_discovery_report(self):
        """–í—ã–≤–µ—Å—Ç–∏ –æ—Ç—á–µ—Ç –æ–± –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–∞—Ö"""
        undiscovered = self.get_new_discoveries()
        
        if not undiscovered:
            logger.info("üì≠ –ù–æ–≤—ã—Ö –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –Ω–µ—Ç")
            return
        
        logger.info(f"\n{'='*80}")
        logger.info(f"üìã –û–¢–ß–ï–¢ DISCOVERY MODE")
        logger.info(f"{'='*80}")
        logger.info(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –Ω–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤: {len(undiscovered)}\n")
        
        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        by_category = {}
        for file_info in undiscovered:
            cat = file_info['category']
            if cat not in by_category:
                by_category[cat] = []
            by_category[cat].append(file_info)
        
        # –í—ã–≤–µ—Å—Ç–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        for category, files in sorted(by_category.items()):
            logger.info(f"üìÅ –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {category} ({len(files)} —Ñ–∞–π–ª–æ–≤)")
            for file_info in files:
                logger.info(f"   ‚Üí {file_info['url']}")
                logger.info(f"      –û–±–Ω–∞—Ä—É–∂–µ–Ω –Ω–∞: {file_info['source_page']}")
            logger.info("")
        
        logger.info(f"{'='*80}\n")
    
    def detect_version_upgrades(self) -> List[Dict]:
        """
        –û–±–Ω–∞—Ä—É–∂–∏—Ç—å –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –≤–µ—Ä—Å–∏–π
        
        Returns:
            –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ–± –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è—Ö:
            - base_name
            - current_version
            - current_url
            - new_version
            - new_url
            - priority
            - category
        """
        logger.info("üîç –ü–æ–∏—Å–∫ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π –≤–µ—Ä—Å–∏–π...")
        
        # –ü–æ–ª—É—á–∏—Ç—å –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º—ã–µ —Ñ–∞–π–ª—ã
        tracked_files = db.get_active_tracked_files()
        
        # –ü–æ–ª—É—á–∏—Ç—å –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
        discovered_files = db.get_undiscovered_files()
        
        if not discovered_files:
            logger.info("üì≠ –ù–µ—Ç –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
            return []
        
        # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å version_detector –¥–ª—è –ø–æ–∏—Å–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π
        updates = detector.find_version_updates(tracked_files, discovered_files)
        
        if updates:
            logger.info(f"üÜï –ù–∞–π–¥–µ–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π –≤–µ—Ä—Å–∏–π: {len(updates)}")
            
            # –õ–æ–≥–∏—Ä–æ–≤–∞—Ç—å –∫–∞–∂–¥–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —á–µ—Ä–µ–∑ alert_system
            for update in updates:
                alert_system.log_version_update(update)
        else:
            logger.info("‚úÖ –û–±–Ω–æ–≤–ª–µ–Ω–∏–π –≤–µ—Ä—Å–∏–π –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ")
        
        return updates
    
    def run_full_discovery_with_version_check(self) -> Dict:
        """
        –ó–∞–ø—É—Å—Ç–∏—Ç—å –ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª Discovery Mode —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –≤–µ—Ä—Å–∏–π
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏:
            - discovered_files: —Å–ø–∏—Å–æ–∫ –Ω–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤
            - version_updates: —Å–ø–∏—Å–æ–∫ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π –≤–µ—Ä—Å–∏–π
            - new_files: –ø–æ–ª–Ω–æ—Å—Ç—å—é –Ω–æ–≤—ã–µ —Ñ–∞–π–ª—ã (–Ω–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è)
        """
        logger.info("\n" + "="*80)
        logger.info("üöÄ –ó–ê–ü–£–°–ö –ü–û–õ–ù–û–ì–û DISCOVERY MODE")
        logger.info("="*80 + "\n")
        
        # 1. –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤ –Ω–∞ –∫–∞–Ω–∞—Ä–µ–π–∫–∞-—Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö
        logger.info("–®–∞–≥ 1/3: –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–Ω–∞—Ä–µ–π–∫–∞-—Å—Ç—Ä–∞–Ω–∏—Ü...")
        discovered_files = self.discover_files()
        
        # 2. –ê–Ω–∞–ª–∏–∑ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ —á–µ—Ä–µ–∑ version_detector
        logger.info("\n–®–∞–≥ 2/3: –ê–Ω–∞–ª–∏–∑ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤...")
        analysis_result = detector.analyze_discovered_files()
        
        version_updates = analysis_result['version_updates']
        new_files = analysis_result['new_files']
        
        # 3. –í—ã–≤–æ–¥ –æ—Ç—á–µ—Ç–æ–≤
        logger.info("\n–®–∞–≥ 3/3: –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–æ–≤...")
        
        if version_updates:
            detector.print_version_updates_report(version_updates)
        
        if new_files:
            logger.info(f"\n{'='*80}")
            logger.info(f"üì¶ –ù–û–í–´–ï –§–ê–ô–õ–´ (–Ω–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è)")
            logger.info(f"{'='*80}")
            logger.info(f"–ù–∞–π–¥–µ–Ω–æ –Ω–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤: {len(new_files)}\n")
            
            for nf in new_files[:10]:  # –ü–æ–∫–∞–∑–∞—Ç—å –ø–µ—Ä–≤—ã–µ 10
                logger.info(f"   üÜï {nf['base_name']} v{nf['version']}")
                logger.info(f"      URL: {nf['url']}")
                logger.info(f"      –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {nf.get('suggested_category', 'unknown')}")
            
            if len(new_files) > 10:
                logger.info(f"\n   ... –∏ –µ—â–µ {len(new_files) - 10} —Ñ–∞–π–ª–æ–≤")
            
            logger.info(f"\n{'='*80}\n")
        
        logger.info("="*80)
        logger.info("‚úÖ DISCOVERY MODE –ó–ê–í–ï–†–®–ï–ù")
        logger.info(f"   –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(discovered_files)}")
        logger.info(f"   –û–±–Ω–æ–≤–ª–µ–Ω–∏–π –≤–µ—Ä—Å–∏–π: {len(version_updates)}")
        logger.info(f"   –ù–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤: {len(new_files)}")
        logger.info("="*80 + "\n")
        
        return {
            'discovered_files': discovered_files,
            'version_updates': version_updates,
            'new_files': new_files
        }


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä discovery
discovery = FileDiscovery()

