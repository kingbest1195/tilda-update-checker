"""
–ú–æ–¥—É–ª—å –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–æ–≤ —Å Tilda CDN
"""
import logging
import hashlib
import time
from typing import List, Dict, Optional, Tuple
from urllib.parse import urlparse

import requests
from requests.adapters import HTTPAdapter
from requests.packages.urllib3.util.retry import Retry

import config
from src.database import db

logger = logging.getLogger(__name__)


class CDNFetcher:
    """–ö–ª–∞—Å—Å –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–æ–≤ —Å Tilda CDN"""
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è fetcher —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ retry"""
        self.session = self._create_session()
    
    def _create_session(self) -> requests.Session:
        """
        –°–æ–∑–¥–∞—Ç—å —Å–µ—Å—Å–∏—é —Å retry –ª–æ–≥–∏–∫–æ–π
        
        Returns:
            –ù–∞—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è —Å–µ—Å—Å–∏—è requests
        """
        session = requests.Session()
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ retry —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
        retry_strategy = Retry(
            total=config.REQUEST_RETRY_COUNT,
            backoff_factor=config.REQUEST_RETRY_DELAY,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["GET"]
        )
        
        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount("http://", adapter)
        session.mount("https://", adapter)
        
        # –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å User-Agent
        session.headers.update({
            'User-Agent': config.USER_AGENT
        })
        
        return session
    
    def get_monitored_files(self, use_db: bool = True) -> List[Dict[str, str]]:
        """
        –ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        
        Args:
            use_db: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –∫–æ–Ω—Ñ–∏–≥ –∏–∑ –ë–î (True) –∏–ª–∏ —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏–π –∏–∑ config.py (False)
        
        Returns:
            –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å URL, —Ç–∏–ø–æ–º —Ñ–∞–π–ª–∞, –∫–∞—Ç–µ–≥–æ—Ä–∏–µ–π –∏ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º
        """
        files = []
        
        if use_db:
            # –î–ò–ù–ê–ú–ò–ß–ï–°–ö–ò–ô –ö–û–ù–§–ò–ì: –ø–æ–ª—É—á–∏—Ç—å –∞–∫—Ç–∏–≤–Ω—ã–µ —Ñ–∞–π–ª—ã –∏–∑ –ë–î
            logger.debug("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ñ–∏–≥–∞ –∏–∑ –ë–î...")
            tracked_files = db.get_active_tracked_files()
            
            for tf in tracked_files:
                files.append({
                    'url': tf.url,
                    'type': tf.file_type,
                    'category': tf.category,
                    'priority': tf.priority,
                    'domain': tf.domain
                })
            
            # –ï—Å–ª–∏ –≤ –ë–î –ø—É—Å—Ç–æ, fallback –Ω–∞ config.py
            if not files:
                logger.warning("‚ö†Ô∏è –ë–î –ø—É—Å—Ç–∞, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è fallback –Ω–∞ config.py")
                use_db = False
        
        if not use_db:
            # –°–¢–ê–¢–ò–ß–ï–°–ö–ò–ô –ö–û–ù–§–ò–ì: –∏–∑ config.py
            logger.debug("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ñ–∏–≥–∞ –∏–∑ config.py...")
            for category, category_config in config.TILDA_MONITORED_FILES.items():
                priority = category_config.get('priority', 'MEDIUM')
                
                for url in category_config.get('files', []):
                    file_type = self._get_file_type(url)
                    domain = self._extract_domain(url)
                    
                    files.append({
                        'url': url,
                        'type': file_type,
                        'category': category,
                        'priority': priority,
                        'domain': domain
                    })
        
        logger.info(f"–ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞: {len(files)}")
        logger.debug(f"–ö–∞—Ç–µ–≥–æ—Ä–∏–∏: {set(f['category'] for f in files)}")
        logger.debug(f"–î–æ–º–µ–Ω—ã: {set(f['domain'] for f in files)}")
        
        return files
    
    def _extract_domain(self, url: str) -> str:
        """
        –ò–∑–≤–ª–µ—á—å –¥–æ–º–µ–Ω –∏–∑ URL
        
        Args:
            url: URL —Ñ–∞–π–ª–∞
            
        Returns:
            –î–æ–º–µ–Ω (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'static.tildacdn.com')
        """
        parsed = urlparse(url)
        return parsed.netloc
    
    def _get_file_type(self, url: str) -> str:
        """
        –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–∏–ø —Ñ–∞–π–ª–∞ –ø–æ URL
        
        Args:
            url: URL —Ñ–∞–π–ª–∞
            
        Returns:
            –¢–∏–ø —Ñ–∞–π–ª–∞ ('js' –∏–ª–∏ 'css')
        """
        if url.endswith('.js'):
            return 'js'
        elif url.endswith('.css'):
            return 'css'
        else:
            # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
            return 'unknown'
    
    def download_file(self, url: str) -> Optional[Tuple[str, int]]:
        """
        –°–∫–∞—á–∞—Ç—å —Ñ–∞–π–ª —Å CDN
        
        Args:
            url: URL —Ñ–∞–π–ª–∞
            
        Returns:
            –ö–æ—Ä—Ç–µ–∂ (—Å–æ–¥–µ—Ä–∂–∏–º–æ–µ, —Ä–∞–∑–º–µ—Ä) –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        try:
            logger.debug(f"–°–∫–∞—á–∏–≤–∞–Ω–∏–µ: {url}")
            
            response = self.session.get(
                url,
                timeout=(5.0, config.REQUEST_TIMEOUT)  # (connect, read)
            )
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ 404 –æ—à–∏–±–æ–∫
            if response.status_code == 404:
                logger.error(f"‚ùå 404 Not Found: {url}")
                db.increment_404_count(url)
                return None
            
            response.raise_for_status()
            
            content = response.text
            size = len(content.encode('utf-8'))
            
            # –°–±—Ä–æ—Å–∏—Ç—å —Å—á–µ—Ç—á–∏–∫ 404 –ø—Ä–∏ —É—Å–ø–µ—à–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–µ
            db.reset_404_count(url)
            
            logger.debug(f"–£—Å–ø–µ—à–Ω–æ —Å–∫–∞—á–∞–Ω–æ: {url} ({size} –±–∞–π—Ç)")
            return content, size

        except requests.exceptions.Timeout:
            logger.error(f"‚è± Timeout –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {url}")
            return None
        except requests.exceptions.HTTPError as e:
            logger.error(f"üåê HTTP –æ—à–∏–±–∫–∞ {e.response.status_code}: {url}")
            return None
        except requests.exceptions.ConnectionError:
            logger.error(f"üîå –û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {url}")
            return None
        except requests.exceptions.RequestException as e:
            logger.error(f"üåê –û—à–∏–±–∫–∞ —Å–µ—Ç–∏: {url} - {e}")
            return None
        except Exception as e:
            logger.error(f"üí• –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ {url}: {e}", exc_info=True)
            return None
    
    def calculate_hash(self, content: str) -> str:
        """
        –í—ã—á–∏—Å–ª–∏—Ç—å SHA-256 —Ö–µ—à —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
        
        Args:
            content: –°–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞
            
        Returns:
            SHA-256 —Ö–µ—à –≤ –≤–∏–¥–µ hex —Å—Ç—Ä–æ–∫–∏
        """
        return hashlib.sha256(content.encode('utf-8')).hexdigest()
    
    def download_all_files(self) -> List[Dict]:
        """
        –°–∫–∞—á–∞—Ç—å –≤—Å–µ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º—ã–µ —Ñ–∞–π–ª—ã —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        
        Returns:
            –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Ñ–∞–π–ª–∞—Ö
        """
        files = self.get_monitored_files()
        results = []
        
        for file_info in files:
            url = file_info['url']
            file_type = file_info['type']
            category = file_info['category']
            priority = file_info['priority']
            domain = file_info['domain']
            
            # –°–∫–∞—á–∞—Ç—å —Ñ–∞–π–ª
            download_result = self.download_file(url)
            
            if download_result:
                content, size = download_result
                content_hash = self.calculate_hash(content)
                
                results.append({
                    'url': url,
                    'type': file_type,
                    'category': category,
                    'priority': priority,
                    'domain': domain,
                    'content': content,
                    'hash': content_hash,
                    'size': size,
                    'success': True
                })
                
                logger.info(f"‚úì [{category}] {url} - {size} –±–∞–π—Ç, hash: {content_hash[:16]}...")
            else:
                results.append({
                    'url': url,
                    'type': file_type,
                    'category': category,
                    'priority': priority,
                    'domain': domain,
                    'content': None,
                    'hash': None,
                    'size': None,
                    'success': False
                })
                
                logger.warning(f"‚úó [{category}] {url} - –æ—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏")
            
            # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            time.sleep(0.5)
        
        success_count = sum(1 for r in results if r['success'])
        logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {success_count}/{len(results)}")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        categories_stats = {}
        for r in results:
            if r['success']:
                cat = r['category']
                categories_stats[cat] = categories_stats.get(cat, 0) + 1
        
        logger.info(f"–ü–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º: {categories_stats}")
        
        return results
    
    def add_file_to_monitoring(self, url: str, category: str = 'unknown',
                              priority: str = 'MEDIUM') -> bool:
        """
        –î–æ–±–∞–≤–∏—Ç—å –Ω–æ–≤—ã–π —Ñ–∞–π–ª –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
        
        Args:
            url: URL —Ñ–∞–π–ª–∞
            category: –ö–∞—Ç–µ–≥–æ—Ä–∏—è —Ñ–∞–π–ª–∞
            priority: –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç —Ñ–∞–π–ª–∞
            
        Returns:
            True –µ—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ –¥–æ–±–∞–≤–ª–µ–Ω
        """
        try:
            from src.version_detector import detector
            
            # –ü–∞—Ä—Å–∏–Ω–≥ URL –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            parsed = detector.parse_file_url(url)
            
            # –ó–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–∞–π–ª –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –Ω–∞—á–∞–ª—å–Ω–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è
            download_result = self.download_file(url)
            
            if not download_result:
                logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–∞–π–ª –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è: {url}")
                return False
            
            content, size = download_result
            content_hash = self.calculate_hash(content)
            
            # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ –ë–î
            tracked_file = db.save_file_state(
                url=url,
                file_type=parsed['file_type'],
                content=content,
                content_hash=content_hash,
                size=size,
                category=category,
                priority=priority,
                domain=parsed['domain']
            )
            
            # –û–±–Ω–æ–≤–∏—Ç—å base_name –∏ version
            session = db.get_session()
            try:
                tf = session.query(db.TrackedFile).filter(
                    db.TrackedFile.id == tracked_file.id
                ).first()
                if tf:
                    tf.base_name = parsed['base_name']
                    tf.version = parsed['version']
                    tf.is_active = 1
                    session.commit()
            except Exception as db_error:
                session.rollback()
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–∞: {db_error}", exc_info=True)
                raise
            finally:
                session.close()

            logger.info(f"‚úÖ –§–∞–π–ª –¥–æ–±–∞–≤–ª–µ–Ω –≤ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥: {url}")
            return True

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞ –≤ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥: {e}", exc_info=True)
            return False
    
    def check_404_errors(self) -> List[Dict]:
        """
        –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ñ–∞–π–ª—ã —Å –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º 404 –æ—à–∏–±–æ–∫
        
        Returns:
            –°–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ —Å 404 –æ—à–∏–±–∫–∞–º–∏
        """
        files_with_404 = db.get_files_with_404_errors(min_count=3)
        
        if not files_with_404:
            logger.info("‚úÖ –§–∞–π–ª–æ–≤ —Å –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–º–∏ 404 –æ—à–∏–±–∫–∞–º–∏ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ")
            return []
        
        logger.warning(f"‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(files_with_404)} —Ñ–∞–π–ª–æ–≤ —Å –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–º–∏ 404 –æ—à–∏–±–∫–∞–º–∏:")
        
        results = []
        for file in files_with_404:
            logger.warning(
                f"   üî¥ {file.base_name or file.url}: "
                f"{file.consecutive_404_count} –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö 404"
            )
            
            results.append({
                'url': file.url,
                'base_name': file.base_name,
                'consecutive_404_count': file.consecutive_404_count,
                'last_404_at': file.last_404_at,
                'category': file.category,
                'priority': file.priority
            })
        
        return results


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä fetcher
fetcher = CDNFetcher()



